{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611a67cb-71c1-4a16-bfef-757ce22f5bd9",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/img/nb_logo.png?raw=1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7457c-15d3-4c69-812a-2bdaa68392c4",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/notebooks/prediction.ipynb)\n",
    "\n",
    "\n",
    "This is a version of the notebook from [Meta Research](https://research.facebook.com/) --- [here](https://github.com/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb) using the output from the [Embeddings notebook](https://colab.research.google.com/github/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/notebooks/embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0883e-7dc9-498b-8e3d-74a7252eb0f9",
   "metadata": {
    "id": "QMoeBQnUCK_E"
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install h5py > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd71977-a769-4855-9e8a-ad28af800193",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "Xs = []\n",
    "\n",
    "for key in embeddings:\n",
    "  scaled_effect = key.split('|')[-1]\n",
    "  ys.append(float(scaled_effect))\n",
    "  Xs.append(embeddings[key])\n",
    "Xs = torch.stack(Xs, dim=0).numpy()\n",
    "print(len(ys))\n",
    "print(Xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0ea3f-0343-434f-a62a-b80d9ef2e1fc",
   "metadata": {
    "id": "vuqI2vnujG88"
   },
   "outputs": [],
   "source": [
    "def read_hdf5(path):\n",
    "\n",
    "    weights = {}\n",
    "\n",
    "    keys = []\n",
    "    with h5py.File(path, 'r') as f: # open file\n",
    "        for key in f.keys():\n",
    "          weights[key] = list(f[key])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5b506-8c45-4d35-9573-68e583c4b737",
   "metadata": {
    "id": "NUkgnuY0qdz0"
   },
   "outputs": [],
   "source": [
    "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\"\n",
    "\n",
    "embeddings = read_hdf5(path=per_protein_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df3f26-5f81-4c2e-972d-707b12d9e9fb",
   "metadata": {
    "id": "_v6l02vzq11W"
   },
   "outputs": [],
   "source": [
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5076fd-2165-4ad7-9863-dc9525c269c3",
   "metadata": {
    "id": "b1KP5dUNtcJb"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07c9f7-c31e-4043-b51a-345c9c6480a8",
   "metadata": {
    "id": "F3BFHMKPq53y"
   },
   "outputs": [],
   "source": [
    "ys = []\n",
    "Xs = []\n",
    "\n",
    "for key in embeddings:\n",
    "  scaled_effect = key.split('|')[-1]\n",
    "  ys.append(float(scaled_effect))\n",
    "  embs = np.array(embeddings[key])\n",
    "  num_na = np.count_nonzero(np.isnan(embs))\n",
    "  Xs.append(torch.from_numpy(embs))\n",
    "\n",
    "Xs = torch.stack(Xs, dim=0).numpy()\n",
    "print(len(ys))\n",
    "print(Xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d5554-38ee-450c-943c-e1776b8cd569",
   "metadata": {
    "id": "86riejVNsD5_"
   },
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Here we choose to follow the Envision paper, using 80% of the data for training, but we actually found that pre-trained ESM embeddings require fewer downstream training examples to reach the same level of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02558393-d156-4b71-a20b-f1b34afb7bf2",
   "metadata": {
    "id": "3FsQodM_sLPy"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e3a7d-d1cd-41d3-9b2e-efc3e557ac1e",
   "metadata": {
    "id": "B0IBi8QDsEib"
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, train_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df3580-1bd1-4516-b633-dcc9f3e8c678",
   "metadata": {
    "id": "MjCKwf3UtrV_"
   },
   "source": [
    "### PCA\n",
    "\n",
    "Principal Component Analysis is a popular technique for dimensionality reduction. Given `n_features` (1280 in our case), PCA computes a new set of `X` that \"best explain the data.\" We've found that this enables downstream models to be trained faster with minimal loss in performance.  \n",
    "\n",
    "Here, we set `X` to 60, but feel free to change it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbc21d-ce18-41f7-9956-43502ee01687",
   "metadata": {
    "id": "q_DfS0BFsG2C"
   },
   "outputs": [],
   "source": [
    "num_pca_components = 100\n",
    "pca = PCA(num_pca_components)\n",
    "Xs_train_pca = pca.fit_transform(Xs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1fc8f-5e15-42ab-be52-bcbd4927b2ef",
   "metadata": {
    "id": "uUMAc-jbtxQq"
   },
   "source": [
    "<a id='viz_embeddings'></a>\n",
    "## Visualize Embeddings\n",
    "\n",
    "Here, we plot the first two principal components on the x- and y- axes. Each point is then colored by its scaled effect (what we want to predict).\n",
    "\n",
    "Visually, we can see a separation based on color/effect, suggesting that our representations are useful for this task, without any task-specific training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec2b47-6487-46f7-b1ee-73f15b1c7aa0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "YiZjjqMOtuRY",
    "outputId": "6df68e14-6923-484e-fc20-9a2db2a1a374"
   },
   "outputs": [],
   "source": [
    "fig_dims = (7, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "sc = ax.scatter(Xs_train_pca[:,0], Xs_train_pca[:,1], c=ys_train, marker='.')\n",
    "ax.set_xlabel('PCA first principal component')\n",
    "ax.set_ylabel('PCA second principal component')\n",
    "plt.colorbar(sc, label='Variant Effect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b9c95-5287-4e65-a4e6-eced1c61e742",
   "metadata": {
    "id": "JcL5Yig1uLcF"
   },
   "source": [
    "### Initialize grids for different regression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed32e8-b209-4e2a-b0dc-3ac9b157884b",
   "metadata": {
    "id": "pkwofW-quMAe"
   },
   "outputs": [],
   "source": [
    "knn_grid = [\n",
    "    {\n",
    "        'n_neighbors': [5, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'leaf_size' : [15, 30],\n",
    "        'p' : [1, 2],\n",
    "    }\n",
    "    ]\n",
    "\n",
    "svm_grid = [\n",
    "    {\n",
    "        'C' : [0.1, 1.0, 10.0],\n",
    "        'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree' : [3],\n",
    "        'gamma': ['scale'],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628bcb5-16a7-4d91-b05b-c7a280d87ba8",
   "metadata": {
    "id": "yCOg4gUiuNax"
   },
   "outputs": [],
   "source": [
    "cls_list = [KNeighborsRegressor(), SVR()]\n",
    "param_grid_list = [knn_grid, svm_grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd2526-ee94-4a07-b578-002913f61ca6",
   "metadata": {
    "id": "nx0gTUW9uRdT"
   },
   "source": [
    "### Run Grid Search\n",
    "\n",
    "(will take a few minutes on a single core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd620f-2ba1-427d-8ee7-bd812e216d14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mc1aOsrKuPR9",
    "outputId": "2175437e-e757-4b31-cde4-86a5d8b3fa4f"
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "grid_list = []\n",
    "for cls_name, param_grid in zip(cls_list, param_grid_list):\n",
    "    grid = GridSearchCV(\n",
    "        estimator = cls_name,\n",
    "        param_grid = param_grid,\n",
    "        scoring = 'r2',\n",
    "        verbose = 1,\n",
    "        n_jobs = -1 # use all available cores\n",
    "    )\n",
    "    grid.fit(Xs_train, ys_train)\n",
    "    result_list.append(pd.DataFrame.from_dict(grid.cv_results_))\n",
    "    grid_list.append(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97871c-0ee0-4309-a0b5-23003d1f9f08",
   "metadata": {
    "id": "qvIBgZg7uVqF"
   },
   "source": [
    "<a id='browse'></a>\n",
    "## Browse the Sweep Results\n",
    "\n",
    "The following tables show the top 5 parameter settings, based on `mean_test_score`. Given our setup, this should really be thought of as `validation_score`.\n",
    "\n",
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d775e-781e-49c1-9209-1fba336cebbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "hJmhU248ucle",
    "outputId": "80b54d89-6d51-41e9-eb80-403c14291998"
   },
   "outputs": [],
   "source": [
    "result_list[0].sort_values('mean_test_score')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be010122-1ab5-482a-8cf9-e63f18594c2f",
   "metadata": {
    "id": "IWjL_HtSuge5"
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf88bb-702e-475b-b29c-dbaf9c499d80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "5srdQco1uhjq",
    "outputId": "33b715f7-e71c-4f3c-ff4a-3c62e046e975"
   },
   "outputs": [],
   "source": [
    "result_list[1].sort_values('mean_test_score')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00030f-6e7a-4319-8072-2e6ba1670ba0",
   "metadata": {
    "id": "fzB3OQAoulSM"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee64f0-25f4-4be1-8ddb-e5f59f41351e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "4P5jGoZKujrW",
    "outputId": "0c0e8cb9-8b83-4765-db09-82af963d9c30"
   },
   "outputs": [],
   "source": [
    "result_list[2].sort_values('mean_test_score')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dbdfd-969f-4c9d-92df-19b469d107b9",
   "metadata": {
    "id": "2ic6oy3ouysk"
   },
   "source": [
    "<a id='eval'></a>\n",
    "## Evaluation\n",
    "\n",
    "Now that we have run grid search, each `grid` object contains a `best_estimator_`.\n",
    "\n",
    "We can use this to evaluate the correlation between our predictions and the true effect scores on the held-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73205590-5023-4d98-aeae-a68288d1e716",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH_baZPSuyG1",
    "outputId": "fbaf0c4b-fe9f-414c-9f7e-e48e5ad3c1a3"
   },
   "outputs": [],
   "source": [
    "for grid in grid_list:\n",
    "    print(grid.best_estimator_.get_params()) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977273a7-ffeb-4bd6-88ab-0d9234de36a1",
   "metadata": {
    "id": "eTJWn1U2u4mH"
   },
   "source": [
    "The SVM performs the best on the `test` set, with a spearman rho of 0.78."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
