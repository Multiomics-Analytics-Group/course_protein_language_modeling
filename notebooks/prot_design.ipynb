{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256c6689-b8e5-4896-a5f5-bf599d10d235",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/img/nb_logo.png?raw=1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02621233-5717-4541-b523-58ab391e3d5b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/notebooks/prot_design.ipynb)\n",
    "\n",
    "\n",
    "This is a version of the notebook from [Martin Pacesa](https://people.epfl.ch/martin.pacesa) --- [here](https://colab.research.google.com/drive/15ucZMtrAeFE_YOBQ9FdrWlAngvljJ4ss?usp=sharing) and [ESMFold](https://github.com/facebookresearch/esm/tree/main/esm/esmfold/v1) -- [here](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3720bcf-7b73-4dd6-b0be-075a0546e596",
   "metadata": {},
   "source": [
    "# ProtGPT2\n",
    "\n",
    "ProtGPT2 ([Ferruz et al 2022](https://www.nature.com/articles/s41467-022-32007-7)) is a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, distantly related to natural ones, as well as unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available through [Hugging Face](https://huggingface.co/nferruz/ProtGPT2).\n",
    "\n",
    "![image.png](../img/protgpt2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbd6fd-9700-41b7-90d4-ff8aba5de9a4",
   "metadata": {
    "id": "QMoeBQnUCK_E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers[torch]\" biopython pymsaviz py3Dmol #> /dev/null\n",
    "!apt-get install muscle #&> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2297f-bce0-4dd1-9c17-40450d79fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up working directories and download files/checkpoints\n",
    "!mkdir protT5 # directory for storing checkpoints, results etc\n",
    "!mkdir protT5/output # directory for storing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358ebea-def6-4f07-88db-7eb3db678b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import transformers\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "from Bio import AlignIO\n",
    "import requests\n",
    "import py3Dmol\n",
    "from pymsaviz import MsaViz, get_msa_testdata\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "# path variables\n",
    "output_path = './protT5/output'\n",
    "\n",
    "#initialise language model\n",
    "protgpt2 = transformers.pipeline('text-generation', model=\"nferruz/ProtGPT2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4fcbb5-f416-4411-baa0-b0271c1df41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "project_name = \"pML_course\"\n",
    "starting_sequence = \"MIQEKDKYVVASVTILESNQ\"\n",
    "sequence_min_length = len(starting_sequence)\n",
    "sequence_max_length = 3 * len(starting_sequence)\n",
    "number_of_generated_sequences = 20\n",
    "\n",
    "#Randomly pick the next amino acid according to its conditional probability distribution? (non-deterministic)\n",
    "do_random_sampling = True\n",
    "\n",
    "#Controls randomness in boltzman distribution. Lower temperature results in less random completions. \n",
    "#As the temperature approaches zero, the model will become deterministic and repetitive. \n",
    "#Higher temperature results in more random completions.\n",
    "sampling_temperature = 1.0\n",
    "\n",
    "#Controlls diversity. 1 means only 1 amino acid with highest probability is considered for each step (token),\n",
    "#resulting in deterministic completions while 950 means 950 amino acids with highest probability are considered at each step. \n",
    "#0 is a special setting meaning no restrictions.\n",
    "top_k_sampling = 950\n",
    "\n",
    "#Controlls diversity. Samples from the smallest possible set of probable amino acids whose cumulative probability exceeds \n",
    "#the probability defined.\n",
    "top_p_filtering = 1.0\n",
    "\n",
    "#Repetition penalty.\n",
    "penalty_for_repetition = 1.2 #@param {type:\"number\"}\n",
    "\n",
    "#Correction factor used for sequence length.\n",
    "#GPT2 measures length in tokens, not amino acids, therefore generated sequences are 3-4 times longer than specified.\n",
    "#You can either use the empirically determined 3.2x correction factor (Default), or none (1), or let the script calculate\n",
    "#the exact factor needed for your specified parameters, however this will result in double the running time.\n",
    "correction_factor = 3.2\n",
    "\n",
    "#set seed for reproducibility\n",
    "seed =  423"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2b4be-8ff7-45fd-996c-b2404d7ef95a",
   "metadata": {},
   "source": [
    "# Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153a924-22af-4d10-a2b7-3e50d89012e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "transformers.set_seed(seed)\n",
    "\n",
    "#correct sequence lengths\n",
    "corrected_sequence_min_length = int(sequence_min_length/correction_factor, )\n",
    "corrected_sequence_max_length = int(sequence_max_length/correction_factor, )\n",
    "\n",
    "#generate sequences\n",
    "sequences = protgpt2(starting_sequence, \n",
    "                     min_length=corrected_sequence_min_length, \n",
    "                     max_length=corrected_sequence_max_length, \n",
    "                     do_sample=do_random_sampling, \n",
    "                     top_k=top_k_sampling, \n",
    "                     top_p=top_p_filtering, \n",
    "                     temperature=sampling_temperature, \n",
    "                     repetition_penalty=penalty_for_repetition, \n",
    "                     num_return_sequences=number_of_generated_sequences, \n",
    "                     batch_size=1, eos_token_id=0)\n",
    "\n",
    "print(\"Sequences generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ffff0-cf8b-46a5-9ff9-e29f6af3a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at the generated sequences\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd5627-94a1-4261-be32-b87fb8bcd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasta file name and path\n",
    "fastaname = f'{project_name}_seed{str(seed)}.fa'\n",
    "fastapath = os.path.join(output_path, fastaname)\n",
    "\n",
    "# init list\n",
    "records = []\n",
    "\n",
    "# for every sequence we generated\n",
    "for idx, sequence in enumerate(sequences):\n",
    "    \n",
    "    # create a Seq record\n",
    "    record = SeqRecord(Seq(sequence['generated_text'].replace('\\n', '')),\n",
    "                       id=str(idx),\n",
    "                       name=project_name +\"_\"+ str(idx),\n",
    "                       description=\"ProtGPT2 generated sequence from \" + starting_sequence)\n",
    "    \n",
    "    # then append to the list\n",
    "    records.append(record)\n",
    "\n",
    "# write them to the fasta file\n",
    "with open(fastapath, \"w\") as output_handle:\n",
    "    SeqIO.write(records, output_handle, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c9bc0-47e9-4631-8d7e-24d04abafa29",
   "metadata": {},
   "source": [
    "# Generating Alignments\n",
    "\n",
    "Generating alignments for the sequences we generated using MUSCLE a software for multiple sequence alignment of protein and nucleotide sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981f0a7-6c18-4571-bed2-0d04e4dfcba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating alignment file path\n",
    "fastaname_aligned = fastapath.replace(\".fa\", \"_aligned.aln\")\n",
    "\n",
    "# using MUSCLE to genrate alignment \n",
    "align_muscle = MuscleCommandline(input=fastapath, out=fastaname_aligned)\n",
    "stdout, stderr = align_muscle()\n",
    "alignment = AlignIO.read(fastaname_aligned, \"fasta\")\n",
    "\n",
    "# write to disk\n",
    "with open(fastaname_aligned, 'w') as output_handle:\n",
    "    AlignIO.write(alignment, output_handle, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518d740-b491-4008-8545-b241af92e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the alignments\n",
    "mv = MsaViz(alignment)\n",
    "fig = mv.plotfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08757a47-b1a6-41b3-9b5d-b23088cb05a3",
   "metadata": {},
   "source": [
    "# Predicting Structure using ESMFold\n",
    "\n",
    "We will use the [ESM Metagenomic Atlas, which provides an API](https://esmatlas.com/about#api) that folds a given protein amino acid sequence with ESMFold (esm.pretrained.esmfold_v1) and returns the predicted structure in PDB file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b39d7-fd60-4444-ab45-94232fe7d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    }\n",
    "\n",
    "# initiate list\n",
    "pdb_files = []\n",
    "\n",
    "for record in records:\n",
    "    \n",
    "    # get the sequence as a string\n",
    "    sequence = str(record.seq)\n",
    "    name = record.name\n",
    "    \n",
    "    # use API\n",
    "    response = requests.post('https://api.esmatlas.com/foldSequence/v1/pdb/', headers=headers, data=sequence)\n",
    "    pdb_string = response.content.decode('utf-8')\n",
    "    \n",
    "    # verbose, response of 200 means success\n",
    "    print(sequence, response)\n",
    "    \n",
    "    # adding time delay as to not overwhelm the server (increase this to encounter less 500 responses)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # filename and path\n",
    "    pdb_filename = f'{name}.pdb'\n",
    "    pdb_path = os.path.join(output_path, pdb_filename)\n",
    "\n",
    "    # save the pdb files\n",
    "    with open(pdb_path, 'w') as f:\n",
    "        f.write(pdb_string)\n",
    "\n",
    "    # append filenames to list for later\n",
    "    pdb_files.append(pdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54811f63-6fa2-4b02-808d-42de04ca5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize each pdb file retrieved from ESMfold using py3Dmol\n",
    "for pdb_file in pdb_files:\n",
    "    \n",
    "    # get index\n",
    "    seq_idx = int(re.findall(r'\\d+', pdb_file)[-1])\n",
    "    # get filename with no ext\n",
    "    filename = re.findall(fr'{project_name}_\\d+', pdb_file)\n",
    "    print(f\"Showing structure: {filename}\")\n",
    "    # retrieve sequences\n",
    "    print(f\"Sequence: {records[seq_idx].seq}\")\n",
    "    \n",
    "    # generate viz\n",
    "    view=py3Dmol.view()\n",
    "    view.addModel(open(pdb_file, 'r').read(),'pdb')\n",
    "    view.zoomTo()\n",
    "    view.setBackgroundColor('white')\n",
    "    #Here we set the visualization style for chain B and C\n",
    "    view.setStyle({'cartoon': {'color':'yellow'}})\n",
    "    view.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
