{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06460c02-2004-436b-aa38-1ae0a7587052",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/img/nb_logo.png?raw=1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf874eb-8949-41dd-a33b-0e55ba2954c6",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/notebooks/embeddings.ipynb)\n",
    "\n",
    "\n",
    "This is a version of the notebook from [SETH](https://github.com/DagmarIlz/SETH) --- [here](https://colab.research.google.com/drive/1vDWh5YI_BPxQg0ku6CxKtSXEJ25u2wSq?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6f0c8f-d9db-4623-a2ee-483008dc9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"transformers[torch]\" sentencepiece h5py biopython > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efcc6ce-9843-4541-a6de-6b3a6ba14c45",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:red\">Important</span></h3> \n",
    "If you are running in Google Colab, change the Notebook settings to use `GPU`.\n",
    "\n",
    "Just follow **Edit** > **Notebook settings** or **Runtime** > **Change runtime type** and select **GPU** as Hardware accelerator.\n",
    "\n",
    "![gpu.png](../img/gpu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75061fa6-eea7-42dc-b057-5da1c56094c0",
   "metadata": {
    "id": "5c0749e1"
   },
   "source": [
    "# Embedding Protein Sequences\n",
    "\n",
    "In this notebook, we will use a pre-trained language model, [ProtT5-XL-UniRef50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50), to encode the protein sequences of 5000+ $\\beta$-$lactamase$ TEM-type varients from FASTA file [P62593.fasta](https://github.com/facebookresearch/esm/blob/2b369911bb5b4b0dda914521b9475cad1656b2ac/examples/data/P62593.fasta). This data was subsetted from a deep mutational scan released by [Gray et al. (2018)](https://www.cell.com/cell-systems/pdfExtended/S2405-4712(17)30492-1). \n",
    "\n",
    "The goal of this notebook is to obtain an embedding (fixed-dimensional vector representation) for each mutated sequence.\n",
    "\n",
    "Although the embedding won't capture all the information from the original data, good embedding representations allow us to analyze, cluster, or use them as features to train machine learning models. \n",
    "\n",
    "The embeddings generated in this notebook will then be used in the next exercise (prediction.ipynb) to train a simple varient predictor (i.e., predict the activity of the protein mutation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6f6b9-0778-4435-a515-e9616522c4b3",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>NOTE</b></p>\n",
    "<p style='margin-left:1em;'>\n",
    "    Even when using GPU, embedding the protein sequences takes some time (~25mins) so to begin go ahead and run all cells of this notebook so that the process is started in the background as we review the notebook.\n",
    "</p>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "    A shortcut to running all cells is going to the \"Runtime\" menu and selecting \"Run all\".\n",
    "\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c4fc2-00ea-4e24-90fa-d96bf3751810",
   "metadata": {
    "id": "5c0749e1"
   },
   "source": [
    "----\n",
    "\n",
    "## The Data: P62593 Sequences\n",
    "\n",
    "To start we will import and explore the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e520014b-391d-41b0-ad62-7d51f56511b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘protT5’: File exists\n",
      "mkdir: cannot create directory ‘protT5/output’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1719k  100 1719k    0     0  3634k      0 --:--:-- --:--:-- --:--:-- 3628k\n"
     ]
    }
   ],
   "source": [
    "# Set up working directories and download files/checkpoints \n",
    "!mkdir protT5 # directory for storing checkpoints, results etc\n",
    "!mkdir protT5/output # directory for storing your embeddings\n",
    "!curl -o P62593.fasta https://dl.fbaipublicfiles.com/fair-esm/examples/P62593.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcfc1b48-24d6-4d6f-86aa-9d3048a6d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Path variables\n",
    "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n",
    "seq_path = 'P62593.fasta' # where the fasta file is saved\n",
    "\n",
    "# check whether GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009ac9f9-b169-4696-abf6-a1c0a3ef8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(fasta_path:str) -> dict:\n",
    "    '''\n",
    "    reads in fasta file and returns a dictionary with primary id/sequence key/value pairs \n",
    "    '''\n",
    "    \n",
    "    # dictionary to append to\n",
    "    seqs = {}\n",
    "    \n",
    "    # read in and parse fasta file\n",
    "    with open(fasta_path) as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            # append each varient to the dict\n",
    "            seqs[record.id] = record.seq\n",
    "\n",
    "    # verbose\n",
    "    example_id=next(iter(seqs))\n",
    "    print(f\"Read {len(seqs)} sequences.\")\n",
    "    print(f\"Example:\\nKey: {example_id}\\nValue: {seqs[example_id]}\")\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa98336-a298-4f5f-87b6-98718ec5e22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 5397 sequences.\n",
      "Example:\n",
      "Key: 0|beta-lactamase_P20P|1.581033423\n",
      "Value: MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW\n"
     ]
    }
   ],
   "source": [
    "# read in file\n",
    "fasta_output = read_fasta(seq_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717503ab-a593-4af2-8128-4e64491c3ebc",
   "metadata": {},
   "source": [
    "In the FASTA file there are 5,397 sequences. As we can see in the example above from our fasta dictionary, each entry contains:\n",
    "\n",
    "- key: `{index}|beta-lactamase_{mutation}|{scaled_varient_effect}`\n",
    "    > in prediction.ipynb we will be predicting the `scaled_varient_effect` value, which describes the scaled effect of the mutation. \n",
    "- value: the mutated $\\beta$-lactamase sequence, where a single residue is mutated (swapped with another amino acid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06a40d-a52f-46bf-bfe7-3b2374eea224",
   "metadata": {},
   "source": [
    "## The Model: ProtT5-XL-UniRef50\n",
    "\n",
    "ProtT5-XL-UniRef50 is based on the t5-3b model and was pretrained on a large corpus of protein sequences in a self-supervised fashion. This means it was pretrained on the raw protein sequences only, with **no humans-in-the-loop labelling** them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those protein sequences.\n",
    "\n",
    "This model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half precision (float16). As such, this model can efficiently be used to create protein/ amino acid representations. When used for training downstream networks/ feature extraction, these embeddings produced the same performance (established empirically by comparing on several downstream tasks). \n",
    "\n",
    "In the following cells we will prepare functions that will later assist us when generating the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072fe5a-3d34-475b-b025-abb7f3eef1f8",
   "metadata": {},
   "source": [
    "### `get_T5_model()` Load encoder-part of ProtT5 in half-precision\n",
    "\n",
    "To start we create a function that will load the model and associated tokenizer. Recall from the previous notebook (model_training.ipynb) where every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, where tokenization for protein language models involve coverting each amino acid to a single token.\n",
    "\n",
    "**Recall: Fine-tuning flow chart from the previous notebook**\n",
    "![Chart of the pretrained model fine-tuning process](../img/fine-tuning.png)\n",
    "\n",
    "\n",
    "This function accomplishes the \"Model Checkpoint Loading\" step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1c2e47-9131-41f6-b39b-2da11b4afe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50) \n",
    "def get_T5_model():\n",
    "    '''\n",
    "    retrieves the model and tokenizer\n",
    "    '''\n",
    "    # specify the encorder-part of the model \n",
    "    model_checkpoint = 'Rostlab/prot_t5_xl_half_uniref50-enc'\n",
    "    \n",
    "    # import the model \n",
    "    model = T5EncoderModel.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    \n",
    "    # import tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404313f-0c46-4cae-acea-5bfdbe86bd99",
   "metadata": {},
   "source": [
    "Let's use the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5282b4cc-36f4-4c70-93f2-5fb32a67f6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab447291ff64ee8beea7183c3b32857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc44789408c7466a950e00dcf12628dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfeefb4abee42f083524507793f9bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccc1a3640a740559aa1f0c70b9f25f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/238k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cd31df2f0e41cb9a1d9f504f37fd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# loading model\n",
    "model, tokenizer = get_T5_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef88997-d8c8-4c62-b394-1d2c3b005a4f",
   "metadata": {},
   "source": [
    "### `get_embeddings()` Using the model to generate the embeddings\n",
    "\n",
    "From the flow chart above, the function `get_embeddings()` includes the 'Tokenization' and 'Dataset Creation' steps. Additionally, the model will encode the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261698df-2497-4f6d-89ef-1df09736110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, tokenizer, seqs, max_seq_len=1000, max_batch=100):\n",
    "    '''\n",
    "    use the encoder to embbed the sequences via batch-processing\n",
    "    -----\n",
    "    \n",
    "    parameters:\n",
    "        model: from get_T5_model()\n",
    "        tokenizer: from get_T5_model()\n",
    "        seqs: the dictionary of sequences generated by read_fasta() \n",
    "        max_seq_length: the upper sequences length for applying batch-processing\n",
    "        max_batch: the upper number of sequences per batch\n",
    "        \n",
    "    returns:\n",
    "        results: a dictionary containing the embedding representations of the sequences\n",
    "    '''\n",
    "\n",
    "    # initialize a dictionary, the embeddings will be accessible from results['protein_embs'] \n",
    "    results = {\"protein_embs\" : dict()}\n",
    "\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
    "    seq_dict = sorted(seqs.items(), \n",
    "                      # 'key' option is a function that serves as a basis of sort comparison.\n",
    "                      key=lambda kv: len(seqs[kv[0]]), \n",
    "                      # sort by descending order\n",
    "                      reverse=True\n",
    "                     )\n",
    "    \n",
    "    # for time tracking\n",
    "    start = time.time()\n",
    "    \n",
    "    # initialize empty list\n",
    "    batch = list()\n",
    "    \n",
    "    # for each item in the dictionary\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict, 1):\n",
    "        \n",
    "        # add space between residues\n",
    "        seq = ' '.join(list(seq))\n",
    "        \n",
    "        # length of sequence with spaces\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        # append to batch list as tuple\n",
    "        batch.append((pdb_id, seq, seq_len))\n",
    "\n",
    "        # creates n-tuple pairs from each element in batch\n",
    "        pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "        \n",
    "        # empty list\n",
    "        batch = list()\n",
    "        \n",
    "        # Data Preparation and Tokenization:\n",
    "\n",
    "        # add_special_tokens adds extra token at the end of each sequence\n",
    "        token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "        \n",
    "        # making the tokenized sequence into a tensor\n",
    "        input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "        \n",
    "        # now making the mask into a tensor\n",
    "        attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "        \n",
    "        # Generate Embedding:\n",
    "        \n",
    "        # using the model to encode the sequence, generating an embedding representation\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "                # verbosity for progress tracking\n",
    "                print(f'Currently, embedding {pdb_id}')\n",
    "        except RuntimeError:\n",
    "            print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
    "            continue\n",
    "            \n",
    "        # Putting together the dataset:\n",
    "        \n",
    "        # slice off padding if any \n",
    "        emb = embedding_repr.last_hidden_state[0,:seq_len]\n",
    "\n",
    "        # average along column\n",
    "        protein_emb = emb.mean(dim=0)\n",
    "\n",
    "        # save the embedding into results dictionary where the key = the fasta file entry header\n",
    "        results[\"protein_embs\"][pdb_id] = protein_emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "    # get time elapsed\n",
    "    passed_time=time.time()-start\n",
    "    avg_time = passed_time/len(results[\"protein_embs\"])\n",
    "    \n",
    "    # final verbose\n",
    "    print('\\n############# EMBEDDING STATS #############')\n",
    "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
    "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
    "        passed_time/60, avg_time ))\n",
    "    print('\\n############# END #############')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccafd2-6032-4e50-be6a-53e98897f922",
   "metadata": {},
   "source": [
    "**NOTE: What are special tokens?** \n",
    "\n",
    "Special tokens aren't present in the input text, but carry important meaning that we want the model to act on. For exmaple (not spevific to our model): \n",
    "- [PAD] Padding token — Added to the end of shorter inputs so that all inputs have the same length. This is because inputs to a neural network model are typically batched and the model operates on entire batches. \n",
    "- [UNK] Unknown token — Used to limit the number of distinct tokens. For example, if we want a vocabulary of at most 1000 tokens but the input text has 1200, then the remaining 200 will be converted to [UNK].\n",
    "    \n",
    "You can read more [here](https://medium.com/@alexkubiesa/special-tokens-in-tensorflow-3c7718dcb0ef).\n",
    "\n",
    "We will move on and use the function to get the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4097b1-8da4-4cd7-9be1-4caf817294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, embedding 0|beta-lactamase_P20P|1.581033423\n",
      "Currently, embedding 1|beta-lactamase_D207D|1.42563125\n",
      "Currently, embedding 2|beta-lactamase_A215A|1.422813331\n",
      "Currently, embedding 3|beta-lactamase_C75C|1.4155315119999998\n",
      "Currently, embedding 4|beta-lactamase_N134N|1.39696596\n",
      "Currently, embedding 5|beta-lactamase_L137L|1.355533136\n",
      "Currently, embedding 6|beta-lactamase_L28L|1.3516090040000002\n",
      "Currently, embedding 7|beta-lactamase_L199L|1.3516090040000002\n",
      "Currently, embedding 8|beta-lactamase_F149F|1.32191175\n",
      "Currently, embedding 9|beta-lactamase_A200A|1.295473865\n",
      "Currently, embedding 10|beta-lactamase_E210E|1.29406548\n",
      "Currently, embedding 11|beta-lactamase_H24H|1.282201552\n",
      "Currently, embedding 12|beta-lactamase_L19L|1.280029666\n",
      "Currently, embedding 13|beta-lactamase_A183A|1.279505214\n",
      "Currently, embedding 14|beta-lactamase_T27T|1.248455477\n",
      "Currently, embedding 15|beta-lactamase_L38L|1.245034082\n",
      "Currently, embedding 16|beta-lactamase_I229I|1.23953749\n",
      "Currently, embedding 17|beta-lactamase_R118R|1.23855076\n",
      "Currently, embedding 18|beta-lactamase_V214V|1.238463943\n",
      "Currently, embedding 19|beta-lactamase_P143P|1.237482227\n",
      "Currently, embedding 20|beta-lactamase_E175E|1.237061839\n",
      "Currently, embedding 21|beta-lactamase_Y103Y|1.236680537\n",
      "Currently, embedding 22|beta-lactamase_Q37Q|1.228947143\n",
      "Currently, embedding 23|beta-lactamase_G142G|1.223102353\n",
      "Currently, embedding 24|beta-lactamase_T126T|1.213890987\n",
      "Currently, embedding 25|beta-lactamase_R63R|1.210016568\n",
      "Currently, embedding 26|beta-lactamase_L223L|1.209258797\n",
      "Currently, embedding 27|beta-lactamase_K232K|1.204146451\n",
      "Currently, embedding 28|beta-lactamase_V117V|1.204146451\n",
      "Currently, embedding 29|beta-lactamase_A133A|1.204146451\n",
      "Currently, embedding 30|beta-lactamase_A170A|1.204146451\n",
      "Currently, embedding 31|beta-lactamase_G114G|1.204146451\n",
      "Currently, embedding 32|beta-lactamase_A123A|1.200392214\n",
      "Currently, embedding 33|beta-lactamase_Q204Q|1.199645504\n",
      "Currently, embedding 34|beta-lactamase_G234G|1.195860291\n",
      "Currently, embedding 35|beta-lactamase_L146L|1.19545104\n",
      "Currently, embedding 36|beta-lactamase_V101V|1.195300092\n",
      "Currently, embedding 37|beta-lactamase_G90G|1.189841882\n",
      "Currently, embedding 38|beta-lactamase_P224P|1.189627145\n",
      "Currently, embedding 39|beta-lactamase_G154G|1.181999031\n",
      "Currently, embedding 40|beta-lactamase_A225A|1.179258391\n",
      "Currently, embedding 41|beta-lactamase_T193T|1.16875245\n",
      "Currently, embedding 42|beta-lactamase_T179T|1.167976192\n",
      "Currently, embedding 43|beta-lactamase_I140I|1.165260346\n",
      "Currently, embedding 44|beta-lactamase_A185A|1.165092705\n",
      "Currently, embedding 45|beta-lactamase_D113D|1.158658061\n",
      "Currently, embedding 46|beta-lactamase_S128S|1.1574785159999998\n",
      "Currently, embedding 47|beta-lactamase_E61E|1.15497924\n",
      "Currently, embedding 48|beta-lactamase_A222A|1.154938626\n",
      "Currently, embedding 49|beta-lactamase_P181P|1.153655812\n",
      "Currently, embedding 50|beta-lactamase_A246A|1.150079227\n",
      "Currently, embedding 51|beta-lactamase_R176R|1.149113002\n",
      "Currently, embedding 52|beta-lactamase_V29V|1.142534117\n",
      "Currently, embedding 53|beta-lactamase_R189R|1.142419155\n",
      "Currently, embedding 54|beta-lactamase_F58N|1.142207789\n",
      "Currently, embedding 55|beta-lactamase_G43G|1.141336861\n",
      "Currently, embedding 56|beta-lactamase_D155D|1.140947862\n",
      "Currently, embedding 57|beta-lactamase_R92R|1.137335478\n",
      "Currently, embedding 58|beta-lactamase_L49L|1.135457842\n",
      "Currently, embedding 59|beta-lactamase_P217P|1.13381469\n",
      "Currently, embedding 60|beta-lactamase_A132A|1.132707897\n",
      "Currently, embedding 61|beta-lactamase_E56E|1.131946574\n",
      "Currently, embedding 62|beta-lactamase_A23A|1.126812329\n",
      "Currently, embedding 63|beta-lactamase_L10L|1.123508365\n",
      "Currently, embedding 64|beta-lactamase_T116T|1.122479724\n",
      "Currently, embedding 65|beta-lactamase_N173N|1.1186516640000002\n",
      "Currently, embedding 66|beta-lactamase_G76G|1.117741507\n",
      "Currently, embedding 67|beta-lactamase_H151H|1.112657025\n",
      "Currently, embedding 68|beta-lactamase_E166E|1.109622262\n",
      "Currently, embedding 69|beta-lactamase_A182A|1.109467777\n",
      "Currently, embedding 70|beta-lactamase_N168N|1.109437862\n",
      "Currently, embedding 71|beta-lactamase_T112T|1.106795641\n",
      "Currently, embedding 72|beta-lactamase_R220R|1.102208855\n",
      "Currently, embedding 73|beta-lactamase_R273R|1.10144642\n",
      "Currently, embedding 74|beta-lactamase_G52G|1.100669144\n",
      "Currently, embedding 75|beta-lactamase_F58F|1.100669144\n",
      "Currently, embedding 76|beta-lactamase_Q88Q|1.100123891\n",
      "Currently, embedding 77|beta-lactamase_L19W|1.094952121\n",
      "Currently, embedding 78|beta-lactamase_T186T|1.0924468040000002\n",
      "Currently, embedding 79|beta-lactamase_S122S|1.092373772\n",
      "Currently, embedding 80|beta-lactamase_V8V|1.092009091\n",
      "Currently, embedding 81|beta-lactamase_T178T|1.090003575\n",
      "Currently, embedding 82|beta-lactamase_A40A|1.088883695\n",
      "Currently, embedding 83|beta-lactamase_D177D|1.086631569\n",
      "Currently, embedding 84|beta-lactamase_D174D|1.084439673\n",
      "Currently, embedding 85|beta-lactamase_V72V|1.082104419\n",
      "Currently, embedding 86|beta-lactamase_D83D|1.08160189\n",
      "Currently, embedding 87|beta-lactamase_T139T|1.078124752\n",
      "Currently, embedding 88|beta-lactamase_K190K|1.077426092\n",
      "Currently, embedding 89|beta-lactamase_L205L|1.07521716\n",
      "Currently, embedding 90|beta-lactamase_I45I|1.07460435\n",
      "Currently, embedding 91|beta-lactamase_A15Y|1.072458669\n",
      "Currently, embedding 92|beta-lactamase_T187T|1.072082887\n",
      "Currently, embedding 93|beta-lactamase_F64F|1.070829876\n",
      "Currently, embedding 94|beta-lactamase_R202R|1.070247723\n",
      "Currently, embedding 95|beta-lactamase_I171I|1.069139543\n",
      "Currently, embedding 96|beta-lactamase_I244I|1.06740524\n",
      "Currently, embedding 97|beta-lactamase_A182M|1.067029036\n",
      "Currently, embedding 98|beta-lactamase_S281F|1.067029036\n",
      "Currently, embedding 99|beta-lactamase_V157K|1.067029036\n",
      "Currently, embedding 100|beta-lactamase_A148M|1.067029036\n",
      "Currently, embedding 101|beta-lactamase_T186C|1.067029036\n",
      "Currently, embedding 102|beta-lactamase_T186W|1.067029036\n",
      "Currently, embedding 103|beta-lactamase_S51D|1.067029036\n",
      "Currently, embedding 104|beta-lactamase_L196N|1.067029036\n",
      "Currently, embedding 105|beta-lactamase_N98Q|1.067029036\n",
      "Currently, embedding 106|beta-lactamase_K109Y|1.067029036\n",
      "Currently, embedding 107|beta-lactamase_A16M|1.067029036\n",
      "Currently, embedding 108|beta-lactamase_L19Q|1.065112695\n",
      "Currently, embedding 109|beta-lactamase_L19A|1.064585721\n",
      "Currently, embedding 110|beta-lactamase_V8S|1.062300049\n",
      "Currently, embedding 111|beta-lactamase_Y103W|1.0618851740000002\n",
      "Currently, embedding 112|beta-lactamase_N50N|1.061565115\n",
      "Currently, embedding 113|beta-lactamase_T262Y|1.060920749\n",
      "Currently, embedding 114|beta-lactamase_I206I|1.05990302\n",
      "Currently, embedding 115|beta-lactamase_K190D|1.0596529609999998\n",
      "Currently, embedding 116|beta-lactamase_F6F|1.059540238\n",
      "Currently, embedding 117|beta-lactamase_T112Q|1.059511151\n",
      "Currently, embedding 118|beta-lactamase_P172P|1.05927624\n",
      "Currently, embedding 119|beta-lactamase_P65P|1.058995065\n",
      "Currently, embedding 120|beta-lactamase_S122M|1.058884685\n",
      "Currently, embedding 121|beta-lactamase_A211D|1.058530502\n",
      "Currently, embedding 122|beta-lactamase_V29N|1.055182672\n",
      "Currently, embedding 123|beta-lactamase_Q274T|1.054812461\n",
      "Currently, embedding 124|beta-lactamase_L196K|1.054812461\n",
      "Currently, embedding 125|beta-lactamase_L19K|1.054812461\n",
      "Currently, embedding 126|beta-lactamase_L160L|1.053874348\n",
      "Currently, embedding 127|beta-lactamase_I171N|1.053548681\n",
      "Currently, embedding 128|beta-lactamase_L199I|1.05306725\n",
      "Currently, embedding 129|beta-lactamase_Q97F|1.05306725\n",
      "Currently, embedding 130|beta-lactamase_Y44Y|1.052278813\n",
      "Currently, embedding 131|beta-lactamase_C121C|1.0517263890000002\n",
      "Currently, embedding 132|beta-lactamase_L19C|1.051597547\n",
      "Currently, embedding 133|beta-lactamase_V29W|1.050740237\n",
      "Currently, embedding 134|beta-lactamase_L192L|1.050530784\n",
      "Currently, embedding 135|beta-lactamase_S233S|1.048897879\n",
      "Currently, embedding 136|beta-lactamase_Q97Y|1.048704173\n",
      "Currently, embedding 137|beta-lactamase_F13M|1.048704173\n",
      "Currently, embedding 138|beta-lactamase_V8Y|1.048113086\n",
      "Currently, embedding 139|beta-lactamase_P60D|1.048065979\n",
      "Currently, embedding 140|beta-lactamase_A280M|1.047482516\n",
      "Currently, embedding 141|beta-lactamase_D33T|1.047482516\n",
      "Currently, embedding 142|beta-lactamase_L196Q|1.047482516\n",
      "Currently, embedding 143|beta-lactamase_A225M|1.047482516\n",
      "Currently, embedding 144|beta-lactamase_H156I|1.047482516\n",
      "Currently, embedding 145|beta-lactamase_H24N|1.047482516\n",
      "Currently, embedding 146|beta-lactamase_L47L|1.04717842\n",
      "Currently, embedding 147|beta-lactamase_A15T|1.04666811\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute embeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfasta_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(model, tokenizer, seqs, max_seq_len, max_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 68\u001b[0m         embedding_repr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# verbosity for progress tracking\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrently, embedding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:2086\u001b[0m, in \u001b[0;36mT5EncoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;124;03m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2086\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:725\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    722\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:339\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    338\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 339\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:284\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 284\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    286\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "results = get_embeddings(model, tokenizer, fasta_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e3350-2791-4773-99f2-e55113f288ea",
   "metadata": {},
   "source": [
    "### `save_embeddings()` Writing the embeddings to a file\n",
    "\n",
    "For our final function, we will write the embeddings to a file. We will load this file into prediction.ipynb to train machine learning models. This is also a copy of the embedding file in the reposition in _data/_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6986186f-93f3-4efe-9597-3c60ce5059a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(emb_dict:dict , out_path:str):\n",
    "    '''\n",
    "    takes the resulting embeddings from get_embeddings() and saves to a compressed h5 file\n",
    "    -----\n",
    "    \n",
    "    parameters:\n",
    "        emb_dict (dict): dictionary that is in results['protein_embs'] \n",
    "        out_path (str): path and filename where the embeddings will be saved\n",
    "    '''\n",
    "    \n",
    "    with h5py.File(str(out_path), \"w\") as hf:\n",
    "        for sequence_id, embedding in emb_dict.items():\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3a1d4-0091-4134-9773-0a901f644a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write embeddings to file\n",
    "save_embeddings(results[\"protein_embs\"], per_protein_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
