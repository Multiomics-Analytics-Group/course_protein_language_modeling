{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3494f2e1-0e62-4d3d-a161-6cff27727f78",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/img/nb_logo.png?raw=1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183c1c8-5e35-4eec-9078-caccab5c4708",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/notebooks/model_training.ipynb)\n",
    "\n",
    "\n",
    "This is a version of the notebook from [Matthew Carrigan](https://huggingface.co/Rocketknight1) --- [here](https://huggingface.co/blog/deep-learning-with-proteins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e617c-c8a7-4715-88d4-06e31e947c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers[torch]\" datasets evaluate biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bb9ee-5369-4696-9475-a55aec462562",
   "metadata": {},
   "source": [
    "*NOTE*: If you're opening this notebook locally, make sure your environment has installed these libraries as well:\n",
    "```shell\n",
    "!pip install jupyter ipywidgets --upgrade\n",
    "!pip install pandas scikit-learn\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a6eb017-b2dc-4dbe-8771-69f1443d89e3",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:red\">Important</span></h3> \n",
    "If you are running in Google Colab, change the Notebook settings to use `GPU`.\n",
    "\n",
    "Just follow **Edit** > **Notebook settings** or **Runtime** > **Change runtime type** and select **GPU** as Hardware accelerator.\n",
    "\n",
    "<img src=\"https://github.com/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/img/gpu.png?raw=1\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3e1b8-5517-4016-b7db-f1bb3bb33d31",
   "metadata": {
    "id": "5c0749e1"
   },
   "source": [
    "# Fine-Tuning Protein Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38b001-2450-4792-8647-31bae0bca58e",
   "metadata": {
    "id": "1d81db83"
   },
   "source": [
    "In this notebook, we are going to use transfer learning to fine-tune a large, pre-trained protein language model to predict **Subcellular location** based on protein sequence. \n",
    "\n",
    "The specific model we are going to use is [ESM](https://github.com/facebookresearch/esm), which is the state-of-the-art protein language model at the time of writing (November 2022) and described in detail in [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
    "\n",
    "There are several ESM-2 checkpoints with differing model sizes. **A checkpoint** is an intermediate dump of a modelâ€™s entire internal state (weights, learning rate, etc.) so that the framework can resume the training from this point whenever desired. \n",
    "\n",
    "Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints are:\n",
    "\n",
    "| Checkpoint name | Num layers | Num parameters |\n",
    "|------------------------------|----|----------|\n",
    "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
    "| `esm2_t36_3B_UR50D`          | 36 | 3B      |\n",
    "| `esm2_t33_650M_UR50D`        | 33 | 650M    |\n",
    "| `esm2_t30_150M_UR50D`        | 30 | 150M    |\n",
    "| `esm2_t12_35M_UR50D`         | 12 | 35M     |\n",
    "| `esm2_t6_8M_UR50D`           | 6  | 8M      |\n",
    "\n",
    "*Note*: larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! Also, note that memory usage for attention during training will scale as `O(batch_size * num_layers * seq_len^2)`, so larger models on long sequences will use quite a lot of memory! We will use the `esm2_t12_35M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c447d8c-00ee-4a3b-b924-ccc36e865828",
   "metadata": {
    "id": "32e605a2"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbf0310-6664-4938-80cf-0d6cbe741f2e",
   "metadata": {
    "id": "a8e6ac19"
   },
   "source": [
    "# Fine-tuning\n",
    "\n",
    "<img src=\"https://github.com/Multiomics-Analytics-Group/course_protein_language_modeling/blob/main/img/fine-tuning.png?raw=1\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f033aa-d819-4e16-86de-feecf3d7b17c",
   "metadata": {},
   "source": [
    "## Subcellular Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d88492-5d1e-4f4a-b09d-0f0642f17105",
   "metadata": {
    "id": "c5bc122f"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57191840-da48-42ae-a5b9-c2eb87d0835f",
   "metadata": {
    "id": "4c91d394"
   },
   "source": [
    "We will gather data from [UniProt](https://www.uniprot.org/) to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences and `labels` will be a list of the category for each sequence (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bc787-ca60-44f5-98e6-1cb23e7e5088",
   "metadata": {
    "id": "c718ffbc"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "organism = 9606\n",
    "fields = \",\".join(['accession', 'sequence', 'cc_subcellular_location'])\n",
    "query = f'((organism_id:{organism}) AND (reviewed:true) AND (length:[80 TO 500]))'\n",
    "params = {\n",
    "    'compressed': 'true', \n",
    "    'fields': fields, \n",
    "    'format':'tsv', \n",
    "    'query':query\n",
    "}\n",
    "query_url = 'https://rest.uniprot.org/uniprotkb/stream?'\n",
    "query_url += urllib.parse.urlencode(params)\n",
    "\n",
    "# take a look \n",
    "print(query_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412dd8d-958c-42da-bc26-0f021c832b7b",
   "metadata": {
    "id": "3d2edc14"
   },
   "source": [
    "We searched for `(organism_id:9606) AND (reviewed:true) AND (length:[80 TO 500])` on UniProt to get a list of reasonably-sized human proteins, then selected the format to TSV and the columns to `Accession`, `Sequence` and `Subcellular location [CC]`, since those contain the data we care about for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae78eb-e690-4574-b110-16f4f6a8a2e3",
   "metadata": {
    "id": "dd03ef98"
   },
   "outputs": [],
   "source": [
    "uniprot_request = requests.get(query_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494ca5e-f712-4b7b-9445-043942bc78a4",
   "metadata": {
    "id": "b7217b77"
   },
   "source": [
    "To get this data into Pandas, we use a `BytesIO` object, which Pandas will treat like a file. If you downloaded the data as a file you can skip this bit and just pass the filepath directly to `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2481a9-b84b-420b-8fab-aadddc5160ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "f2c05017",
    "outputId": "9de817df-8a18-4ac3-b70b-452d8f6aa71a"
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "# read in content to dataframe\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "df = pd.read_csv(bio, compression='gzip', sep='\\t')\n",
    "\n",
    "# check out first 5 rows of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5595c8-f8f1-42c3-aa53-cd332d7d38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn more about the df via .info() or .shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650dc9bf-47d7-4faa-98eb-2a40da2e1938",
   "metadata": {
    "id": "0bcdf34b"
   },
   "source": [
    "Nice! Now we have some proteins and their subcellular locations. Let's start filtering this down. First, let's ditch the columns without subcellular location information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badb1af-f4b8-46be-91cd-a75cb7a4c527",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2291e81-9d2b-4411-8305-62a0a058ebfe",
   "metadata": {
    "id": "31d87663"
   },
   "outputs": [],
   "source": [
    "# Drop proteins with missing columns\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f5b8b-455a-4c52-a47b-e3fc14467079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e56bd-dabc-4b2e-8f82-7a6da8538bed",
   "metadata": {
    "id": "10d1af5c"
   },
   "source": [
    "Now we'll make one dataframe of proteins that contain `cytosol` or `cytoplasm` in their subcellular localization column, and a second that mentions the `membrane` or `cell membrane`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27cb39-8c94-4488-a446-7d0e9a031f23",
   "metadata": {
    "id": "c831bb16"
   },
   "outputs": [],
   "source": [
    "# first creating boolean series\n",
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3c8a5-a041-4566-994d-5601edfcc5c4",
   "metadata": {},
   "source": [
    "We ensure that there is no overlap between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7dec8-3f0e-4464-8997-184d9e580c90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "f41139a2",
    "outputId": "05884511-1ce7-435b-b5f5-16cf91ca973c"
   },
   "outputs": [],
   "source": [
    "# use above booleans to filter df\n",
    "# NOTE: using .copy() when slicing/filtering an existing dataframe \n",
    "#       prevents the triggering of 'SettingWithCopyWarning' \n",
    "cytosolic_df = df[cytosolic & ~membrane].copy()\n",
    "\n",
    "# check using .head() and/or .tail()\n",
    "cytosolic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8ade4-d39e-48a1-ad5a-7908faeb9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a996f-ca49-4f34-ac85-15b51ef09826",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "be5c420e",
    "outputId": "2588389b-05e0-4707-8e93-9995880b2dee"
   },
   "outputs": [],
   "source": [
    "# use boolean series to filter df\n",
    "membrane_df = df[membrane & ~cytosolic].copy()\n",
    "\n",
    "# check\n",
    "membrane_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0596b8-6655-4000-a8f6-555b961b84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "membrane_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798179a-96ac-42d3-959e-79cd9564a63c",
   "metadata": {
    "id": "77e8cea6"
   },
   "source": [
    "Now, let's add labels. We use `0` as the label for cytosolic proteins and `1` as the label for membrane proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5bbdb-6f11-4d9e-995a-2019f1338b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding label columns\n",
    "cytosolic_df['label'] = 0\n",
    "membrane_df['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430cec6-fb0c-45db-8854-52e9d37014a2",
   "metadata": {
    "id": "5a4bbda2"
   },
   "source": [
    "Now we will combine the 2 groups to form our initial dataset. Don't worry - the proteins will get shuffled when splitting the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589d77f-7c63-4b5c-b584-8a4ca8f3d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining dataframes and only keeping 'Sequence' and 'label' columns\n",
    "df_sequences = pd.concat([cytosolic_df, membrane_df])[['Sequence', 'label']]\n",
    "\n",
    "# check\n",
    "df_sequences.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edbec8f-e266-4a47-92b5-5679eba48aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution\n",
    "df_sequences['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdd50c-9531-4baa-a9a5-5e8e66521fc0",
   "metadata": {},
   "source": [
    "We can also see that we will be working with a dataset with a balanced number of observations per class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6f5a5-416b-4e74-8f11-ff574ceaf9b6",
   "metadata": {
    "id": "e0aac39c"
   },
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c00e81-f9b2-44af-b798-9b9029c0a5fc",
   "metadata": {
    "id": "a9099e7c"
   },
   "source": [
    "We need to split the data into train and test sets. We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8a8bb-658c-42ad-a6b4-03cea93e6b13",
   "metadata": {
    "id": "366147ad"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    df_sequences['Sequence'], \n",
    "    df_sequences['label'],\n",
    "    test_size=0.2,\n",
    "    shuffle=True, #default\n",
    "    stratify=df_sequences['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fcd94a-31be-4771-9a94-15de903183c2",
   "metadata": {},
   "source": [
    "### Quick EDA of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14faf07e-a4ef-4a24-a31e-2e532afad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "# recall this function from seq_analysis.ipynb\n",
    "# calculates mean amino acid composition for a set of sequences\n",
    "def calculate_mean_AA_composition(sequences):\n",
    "    stats = []\n",
    "    for sequence in sequences:\n",
    "        protein_sequence = ProteinAnalysis(sequence)\n",
    "        st = {k:v/len(sequence) for k,v in protein_sequence.count_amino_acids().items()}\n",
    "        stats.append(st)\n",
    "    df = pd.DataFrame.from_dict(stats).transpose()\n",
    "    df = df.T.mean().reset_index()\n",
    "    df.columns = ['amino acid', 'mean']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e6845-85b1-490b-b1fd-420097446346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = calculate_mean_AA_composition(train_sequences)\n",
    "test_df = calculate_mean_AA_composition(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516588d-974d-4974-a142-b55b78cf6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'colab'\n",
    "\n",
    "fig = px.bar(train_df.sort_values(by='mean'), x='amino acid', y='mean', color='mean')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe273e-cd36-4847-8123-d59c40c8d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(test_df.sort_values('mean'), x='amino acid', y='mean', color='mean')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bc7e9-d8a2-4edf-83a0-58b94c0eddda",
   "metadata": {
    "id": "7d29b4ed"
   },
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ea3fe-1a7d-41f6-b214-4ef3a3c1fea5",
   "metadata": {
    "id": "c02baaf7"
   },
   "source": [
    "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
    "\n",
    "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different. Let's get our tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c56d6-f03b-4306-80f5-0678b051a822",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "48ea66931f1e4eec9c02d44766897106",
      "799fbc24f6d745a89716caaf7e2db87a",
      "62f8ae11792145bea1b1d7afd1497415",
      "e151623066004c969cb6258f70adcde9",
      "994d3df7154e403fb1e29185bb73d714",
      "b6cf9d5576c14031a9fc901bf1c83284",
      "0d4c7cec0152401e9faa3145bef7cbb0",
      "69d9d37fdee24c7cacac2c4a88df0273",
      "8ecac32695454023a98d889893340a4d",
      "008851fc6aec47e6b9294343f281d4ef",
      "881d61da2cc64f0aba9cd11d99d18044",
      "f6e1a4cbf15249628dae2f4c2a51f8cf",
      "d74c66158e8343988e937573a5254c4c",
      "db52d20520bf44798ce76323e4324b80",
      "ef065a30ea9246dfb9b7e4ac96b58ca9",
      "591e3f5003b2422e8aa55ab750226195",
      "7b51332c344a4d73a13327d0448f499f",
      "ef8c06dbfc70495d9d38ffbbf83aeb8d",
      "df6b40fa9fb84c788b860bd3a8ec814e",
      "ea16a9e174e74dcb98b9a5b5229725b5",
      "4a15581db7dd4f8cbf1bd92493ff9587",
      "6e5a3b7598cb466fb327ff99f8c2331c",
      "1769f6420fb845c9935c0cecc4f14189",
      "04b87e010c234a518bf102462b65c758",
      "6d486f76da1e422e83943af3143bc567",
      "fc4e48d289104bfea7348a3e8f74bf13",
      "c41e121a002b4235b6f9dc35ea0bd5f0",
      "64e999c2e40041f7beb5baf4c3a8fefb",
      "5f5a22e1a2c043bf9cb02816628c1f49",
      "3aa54d54e9da433c91b35983e50795ab",
      "635d2a69bb9648e68fc161d2927da091",
      "ddbb89e848a343f29b28d38ea7aa0e39",
      "68fd265ae06542299ea9a1ec664537b5"
     ]
    },
    "id": "ddbe2b2d",
    "outputId": "41950c4a-22fc-4018-838c-fbbcbc421482"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e3399-4c54-48c7-ad0e-71ee5fb57e9c",
   "metadata": {
    "id": "9d16be37"
   },
   "source": [
    "Let's try a single sequence to see what the outputs from our tokenizer look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc349a74-8b06-40a5-9889-65b144ef763d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "687386af",
    "outputId": "a55bc023-b94b-4ef3-fb34-cac712bdf9cf"
   },
   "outputs": [],
   "source": [
    "tokenizer(train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00830f7d-ddc6-4fcb-a7b9-d4193c3d2f6d",
   "metadata": {
    "id": "9a719808"
   },
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781e8d3-8f07-469e-a651-147a02f1cba8",
   "metadata": {
    "id": "56e26ddf"
   },
   "outputs": [],
   "source": [
    "# tokenize train and test sequences, which need to be list of str\n",
    "train_tokenized = tokenizer(train_sequences.tolist())\n",
    "test_tokenized = tokenizer(test_sequences.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8646c13-b638-4b49-928c-23a1caa4e5a1",
   "metadata": {
    "id": "df3681d1"
   },
   "source": [
    "## Training Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec84852-18b3-43db-a953-397997cf7d7b",
   "metadata": {
    "id": "85089e49"
   },
   "source": [
    "Now we want to turn this data into a dataset that PyTorch can load samples from. We can use the HuggingFace `Dataset` class for this, although if you prefer you can also use `torch.utils.data.Dataset`, at the cost of some more boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1fde6-00c6-4930-b2d4-2b21038dc725",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb79ba6c",
    "outputId": "5c355205-9071-4994-bd63-af320efc97bb"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "# check\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2a6c0-efbf-4b9f-ac11-2bafc6777ff5",
   "metadata": {
    "id": "9e809e47"
   },
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697fdbb-e43b-4d48-b481-4e3e90ee97f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "090acc0d",
    "outputId": "a14a5583-d946-47f1-d0a6-b31514d7325e"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "# check\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d38664-d28d-4e70-97c7-e055fe4ed90f",
   "metadata": {},
   "source": [
    "We will split the training set further to use a subset of the data to evaluate the model during fine-tuning (i.e., validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ffd19-2bef-436e-8b26-f5a75ff6ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = train_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# check\n",
    "train_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815741ed-b125-4661-825f-6f126272fdde",
   "metadata": {
    "id": "ced9aaa8"
   },
   "source": [
    "Looks good! We're ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2901c-0d02-473e-baa6-d3bdcaa9dd42",
   "metadata": {
    "id": "af074a5c"
   },
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd5289-8d5a-4977-b49e-d8deeb072795",
   "metadata": {
    "id": "ccab5d70"
   },
   "source": [
    "Next, we want to load our model. Make sure to use exactly the same model as you used when loading the tokenizer, or your model might not understand the tokenization scheme you're using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7b3c8-a150-4552-8e2e-8913563ad214",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570,
     "referenced_widgets": [
      "cd93caa975e74791aba3ede7889c058c",
      "8f7654f0423b48668a983802e6a6aa44",
      "8b9f0a93dcfa4ce08f831355bc82c861",
      "4385c61c3462447c9f6d5a53298fb237",
      "55213ae044de4d9582bcae84c8da9ef6",
      "7908d984799d4d65a398c03346e26729",
      "03618d81301b47ba83336698ffa9c2b7",
      "b003ddf9b40946d690681825979f8205",
      "56e04648858b4965ad94e32c3b26a2f6",
      "cae4138fb556439a994edefcce4e6bd3",
      "e0a77469998a47bd9f39a28403c5ec6c",
      "901a5c1ebca34d899b203035d23c0f46",
      "ce7b83f8fa9649d7b11cce43001198bb",
      "8ede690fe0874410b347a48c276915e7",
      "91c00e196504498a9e5c2d4391b4c0f7",
      "f177f5bd5dfa4765b6eb6bd8ae091348",
      "ec71fad4a3cd4815aff289e25ef70de2",
      "9949f236e90b45c2ba921ce479e54920",
      "b0d02c8c223346b5bf9ab6c7569b9eff",
      "20764cdac47445c09985f2204024d7f6",
      "71f7ff0a7df84d0898dbf14ac3808165",
      "406d2f83ccad49ec9cb114f78d8514a9",
      "8a5fbf0a1e6042f3b1d3900a1d854c2a",
      "8e94130dc4a64b56a156d1bff4c90131",
      "e6c6841a50474f7daef5b1ec9a0f1b79",
      "37329fa50e0a4f3fbb2f863fc9150a40",
      "039c8434f6564960a846b49998e6dc98",
      "8697aee2b4fe469eb8b2acf4c93934cc",
      "ecdf7a23e3be42b1a98d6206221a005d",
      "641ba6628d1c43138343987f6a102a7c",
      "8e3718c5e1e846bca36cacc19f481d7b",
      "30fedea993de40e5962816ded086afa7",
      "a2b96cc7bf62480f8c08e49a79c54e91",
      "d906e65029ec46cca5d4c8736c790535",
      "6360234a90c44caa8d9bbf7b6833fc9f",
      "57893132e5be4b2baca0db3dadd46999",
      "eafbb78481fb41f8a9792eb5e8a00043",
      "fd88fadcbd9e449f9ba6938013d58f7c",
      "290af5dfc2b3470b94e07a96d982c6b1",
      "0384b6458894415a95142dd26902e47f",
      "4434eef7462e44b8a77ebfa30b15de19",
      "262dfa94f2404e3caf3c3bb81215eedf",
      "9a5a74ef3a294b2cbaeb3a2968bffef9",
      "c0b5bfdf1f6247dfb4e967c750fdd16d",
      "ee83891aacff4186a2e5b1c015d5aa48",
      "b9da13013ccb471fb46066fd37216ec1",
      "cb854268a04c4857b8f641575150257a",
      "f066f129604e47bb975acd5f2fb380f9",
      "46a7dc72b34c4a80b4df01d7e2130412",
      "0b3b862d04c14d61af5b08ad2c00895b",
      "affc17d525b245a9a7df95f3ed5ecce5",
      "16f52a411b3c4f10b986d85de862e34b",
      "3f8103fe9c5e49449c0e39f4f1603f02",
      "fb46882705784e1fb516ce156fa8f86e",
      "0c0c429e800041bfa1530080d7b26400",
      "f8bf3aebff25499fae5de8d82704fb6f",
      "9ea7802e962e4647acf36aa0a42a86d2",
      "28b1f101ef3749819f86b69349ce6c24",
      "dcf594b86fce4062b0e624e6b7d996aa",
      "7cb1942781ba4a6e9619d2cfca17dd5b",
      "d6ba55fbcc20433a84a79478acd4f1ff",
      "2d4f570d845540f8bbcce3a40d0026a7",
      "ef7cbb1b37f84c208897c447ed6612da",
      "559694d417154050b2712a6441e56f7d",
      "758e1a01f3bd4c79a6833560c22e03f9",
      "dfe14fbaae0e484f9bec2dd4bcb414ca",
      "e4ca5fcd11964869901ed46cdfb4479a",
      "82583bb78f6c446a9b374db27a1c262e",
      "79c8f3195f0f4e3fb07c045cef8504de",
      "7890cb3c089148bf9f63d9d45fc7b41f",
      "95e819c3b93d443595704b14a6aceebc",
      "7d2248b1c05644db9cc3b44e31a46cc1",
      "73d0ece7c54d4e339d2d5bac0855aa58",
      "89a06e969be9432d9294f9c7ff9b8675",
      "6efb8d2b82c54e828de047e2c2b29efd",
      "44b959e9df0b438799d99d57f6e313a6",
      "4c6a54049338417e811f39d9ea54b33f"
     ]
    },
    "id": "fc164b49",
    "outputId": "032629cb-2e91-478d-fd25-c6a179b3f26b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# the number of classes \n",
    "num_labels = train_labels.nunique() #2 \n",
    "\n",
    "# instantiate the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ed91f-87ef-49f6-90b0-fea5ba491afb",
   "metadata": {
    "id": "49dcba23"
   },
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
    "\n",
    "Next, we initialize our `TrainingArguments`. These control the various training hyperparameters, and will be passed to our `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b6f44-ec76-486e-b5d7-f861d1b254e2",
   "metadata": {
    "id": "775cb3e7"
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-localization\",\n",
    "    evaluation_strategy = \"epoch\", #evaluate at end of every epoch\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9149a-0c08-4c73-b5e8-5af49109ae13",
   "metadata": {
    "id": "bc95d099"
   },
   "source": [
    "Next, we define the metric we will use to evaluate our models and write a `compute_metrics` function. We can load this from the `evaluate` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbfcc1-3175-4693-9b2f-a5f266c4bff7",
   "metadata": {
    "id": "471cef9f"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "# we will compute the model accuracy\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "# build evaluation function to be used during model training\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104367a-5c18-4d95-9950-39d1cdc85345",
   "metadata": {
    "id": "709dcf25"
   },
   "source": [
    "## Training\n",
    "\n",
    "And at last we're ready to initialize our `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e0744-bac2-42db-9236-58b5e772587e",
   "metadata": {
    "id": "e212b751"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_val_dataset['train'],\n",
    "    eval_dataset=train_val_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700b0cd-99db-46f5-a39b-aba46c1ce145",
   "metadata": {
    "id": "32924d0d"
   },
   "source": [
    "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it one last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive samples like the dictionaries seen above and will need to return a dictionary of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bd8fe-42ff-4f3b-a0ed-f0f401a68fbd",
   "metadata": {
    "id": "72f7a24c"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e341ad-9594-49bb-a2f3-fec4e68ea799",
   "metadata": {
    "id": "9c3cf6da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec662d9-ec7f-4313-b365-3299060dc09e",
   "metadata": {
    "id": "dfec59f4"
   },
   "source": [
    "Nice! After three epochs we have a model accuracy of ~94%. \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "We will take a look at how our final fine-tuned model performs when predicting the subcellular location of protein sequences the model has not seen before (i.e., test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c93f7-aa89-4b2f-8233-d8b1e73199dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at model outputs\n",
    "output = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad25398-619b-44ea-9a07-d0f4cc4ad998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from logits to classifications\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "# from logits to class probabilities\n",
    "y_proba = softmax(output.predictions, axis=-1)\n",
    "\n",
    "# from probabilities to classifications\n",
    "y_pred = np.argmax(y_proba, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004775ce-f71e-4122-800e-cb63c6438eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=test_labels, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e2ed8-4fb6-40e6-87c3-ce2ed526934c",
   "metadata": {},
   "source": [
    "Our goal was to classify the subcellular location of given proteins. Our final fine-tuned model achieved 95% on the test set. Note that we didn't do a lot of work to filter the training data or tune hyperparameters for this experiment, and also that we used one of the smallest ESM-2 models. With a larger starting model and more effort to ensure that the training data categories were cleanly separable, accuracy could almost certainly go a lot higher!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
